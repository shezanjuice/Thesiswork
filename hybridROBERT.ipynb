{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffae124-9f3f-4b36-a1fe-8823c347f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\thesiswork\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "32563/32563 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9190"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel, DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_available_memory():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        return details.get('memory_limit', 0) / (1024 ** 2)\n",
    "    return 8000  \n",
    "\n",
    "def get_dynamic_batch_size():\n",
    "    free_mem = get_available_memory()\n",
    "    return 16 if free_mem > 16000 else 8 if free_mem > 8000 else 4 if free_mem > 4000 else 2\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "tf.config.optimizer.set_jit(False)\n",
    "BATCH_SIZE = get_dynamic_batch_size()\n",
    "MAX_LENGTH = 96\n",
    "\n",
    "data = pd.read_csv('datasetofsenti.csv').drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", str(x).lower().strip()))\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data[\"text\"], data[\"label\"], test_size=0.375, random_state=42\n",
    ")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_labels)\n",
    "y_test = label_encoder.transform(test_labels)\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(texts):\n",
    "    return {\n",
    "        'roberta_input_ids': roberta_tokenizer(\n",
    "            texts.tolist(), max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors='tf'\n",
    "        )['input_ids'],\n",
    "        'distilbert_input_ids': distilbert_tokenizer(\n",
    "            texts.tolist(), max_length=MAX_LENGTH, padding='max_length', truncation=True, return_tensors='tf'\n",
    "        )['input_ids']\n",
    "    }\n",
    "\n",
    "def create_dataset(encodings, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {'roberta_input': encodings['roberta_input_ids'],\n",
    "         'distilbert_input': encodings['distilbert_input_ids']},\n",
    "        labels\n",
    "    )).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_dataset = create_dataset(tokenize(train_texts), y_train)\n",
    "test_dataset = create_dataset(tokenize(test_texts), y_test)\n",
    "\n",
    "def build_hybrid_model(num_classes):\n",
    "    roberta_input = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='roberta_input')\n",
    "    distilbert_input = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='distilbert_input')\n",
    "\n",
    "    roberta_out = roberta_model(roberta_input, training=True).last_hidden_state[:, 0, :]\n",
    "    distilbert_out = distilbert_model(distilbert_input, training=True).last_hidden_state[:, 0, :]\n",
    "    \n",
    "    combined = Attention()([tf.expand_dims(roberta_out, 1), tf.expand_dims(distilbert_out, 1)])\n",
    "    combined = tf.squeeze(combined, axis=1)\n",
    "    outputs = Dense(num_classes, activation='softmax')(combined)\n",
    "    \n",
    "    return Model(inputs=[roberta_input, distilbert_input], outputs=outputs)\n",
    "\n",
    "model = build_hybrid_model(len(label_encoder.classes_))\n",
    "model.compile(optimizer=Adam(2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=3),\n",
    "        ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "    ],\n",
    "    steps_per_epoch=len(train_texts) // (BATCH_SIZE * 4)\n",
    ")\n",
    "\n",
    "model.load_weights('best_model.keras')\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9624eb-2b1b-4fa5-a90f-52098bb97879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
